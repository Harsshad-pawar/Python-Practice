{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HARSSHAD PAWAR\n",
    "Assignment02:\n",
    "use twitter dataset and perform below activities on text feature:\n",
    "1. OHE\n",
    "2. BOW\n",
    "3. N-GRAM like 2-gram, 3-Gram, 4-Gram\n",
    "4. tf-idf\n",
    "\n",
    "Write 5 advantages and disadvantages of\n",
    " OHE VS BOW VS N-GRAM VS TF-IDF\n",
    "\n",
    "Note : Before we perform Encoding on text feature 1st we need to have text preprocessing done on text feature. \n",
    "\n",
    "Hence in my code PART 1 : Preprocessing and PART 2 is Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1 : Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from the Kaggle dataset\n",
    "import pandas as pd\n",
    "import re\n",
    "data=pd.read_csv(\"testdata/twcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>419673</th>\n",
       "      <td>478495</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>False</td>\n",
       "      <td>Fri Dec 01 12:40:34 +0000 2017</td>\n",
       "      <td>@228686 Hello- Can you explain further what yo...</td>\n",
       "      <td>478496,478497</td>\n",
       "      <td>478498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121903</th>\n",
       "      <td>2282854</td>\n",
       "      <td>663495</td>\n",
       "      <td>True</td>\n",
       "      <td>Sat Nov 11 21:50:16 +0000 2017</td>\n",
       "      <td>@123813 el capitulo 2 de Dos Lagos cuándo lo s...</td>\n",
       "      <td>2282852</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420773</th>\n",
       "      <td>479697</td>\n",
       "      <td>ATVIAssist</td>\n",
       "      <td>False</td>\n",
       "      <td>Fri Dec 01 12:48:54 +0000 2017</td>\n",
       "      <td>@229001 Hello There Omar sorry for the delay, ...</td>\n",
       "      <td>479698</td>\n",
       "      <td>82869.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456377</th>\n",
       "      <td>2625728</td>\n",
       "      <td>741683</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 00:40:39 +0000 2017</td>\n",
       "      <td>@AmericanAir I did.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2625727.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944244</th>\n",
       "      <td>2099185</td>\n",
       "      <td>marksandspencer</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Nov 08 09:30:49 +0000 2017</td>\n",
       "      <td>@620035 Hi Abbie, could you DM us your email a...</td>\n",
       "      <td>2099186</td>\n",
       "      <td>2099187.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id        author_id  inbound                      created_at  \\\n",
       "419673     478495       AmazonHelp    False  Fri Dec 01 12:40:34 +0000 2017   \n",
       "2121903   2282854           663495     True  Sat Nov 11 21:50:16 +0000 2017   \n",
       "420773     479697       ATVIAssist    False  Fri Dec 01 12:48:54 +0000 2017   \n",
       "2456377   2625728           741683     True  Tue Oct 31 00:40:39 +0000 2017   \n",
       "1944244   2099185  marksandspencer    False  Wed Nov 08 09:30:49 +0000 2017   \n",
       "\n",
       "                                                      text response_tweet_id  \\\n",
       "419673   @228686 Hello- Can you explain further what yo...     478496,478497   \n",
       "2121903  @123813 el capitulo 2 de Dos Lagos cuándo lo s...           2282852   \n",
       "420773   @229001 Hello There Omar sorry for the delay, ...            479698   \n",
       "2456377                                @AmericanAir I did.               NaN   \n",
       "1944244  @620035 Hi Abbie, could you DM us your email a...           2099186   \n",
       "\n",
       "         in_response_to_tweet_id  \n",
       "419673                  478498.0  \n",
       "2121903                      NaN  \n",
       "420773                   82869.0  \n",
       "2456377                2625727.0  \n",
       "1944244                2099187.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start with the lowercase conversion:\n",
    "# 2.  lowercase / uppercase\n",
    "\n",
    "data[\"text\"]=data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now remove html tags and urls:\n",
    "import re\n",
    "def remove_html_tags(text1):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(\"\",text1)\n",
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943318    @619835 hello there, tito. i noticed your twee...\n",
       "1040618    @391111 please dm us your tracking number, and...\n",
       "576964     i have ordered 3 monitors over the past five y...\n",
       "1229598    @436125 this is like music to our ears! thank ...\n",
       "2715053    @324984 @801248 @applesupport @104870 this yea...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will be removing Emojis:\n",
    "# 3.  Emojis\n",
    "\n",
    "import emoji\n",
    "def remove_emoji(text):\n",
    "    clean_text=emoji.demojize(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1962769    @624420 hey! our curation team are independent...\n",
       "2627924    @781234 can you dm us so we can look into this...\n",
       "454559           @upshelp sent you a dm with no response....\n",
       "369557     @215608 sorry for the trouble caused, your con...\n",
       "2220641    @687216 please fill in your details using the ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will remove the punctuation:\n",
    "#4.  punctuation\n",
    "\n",
    "import string\n",
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,\"\")\n",
    "    return text\n",
    "\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048349             392983 sorry we are now following youtom\n",
       "2042899    chasesupport how are people supposed to get cr...\n",
       "2579362    applesupport thanks  what password does icloud...\n",
       "887284     virgintrains no at seat service in 1st class a...\n",
       "1220447    433909 thats ok were here to answer your apple...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will update abbravation or slang words:\n",
    "# 7.  abbravation\n",
    "\n",
    "chat_words={\n",
    "   \" AFAIK\":\"As Far As I Know\",\n",
    "\"AFK\": \"Away From Keyboard\",\n",
    "\"ASAP\":\"As Soon As Possible\",\n",
    "\"BTW\":\"By The Way\",\n",
    "\"B4\":\"Before\",\n",
    "\"LAMO\":\"Laugh My A.. Off\",\n",
    "\"FYI\":\"For your information\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    \n",
    "    new_text=[]\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"text\"] = data[\"text\"].apply(lambda x: ' '.join(x))\n",
    "data[\"text\"] = data[\"text\"].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220240     176886 team 116447 our crews continue to work ...\n",
       "2124105    spotifycares i was charged for premium but do ...\n",
       "523619     259319 marksandspencer how fab gorgeous girl t...\n",
       "732988     verizonsupport finally got a support person on...\n",
       "249657     184578 hi there id be happy to help whats the ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rectify spelling mistakes:\n",
    "# 6.  spelling mistakes\n",
    "\n",
    "# The TextBlob library's spell correction function can be computationally expensive, especially for large datasets. Instead of applying the correction directly to each text entry in your dataframe using the lambda function, you can speed up the process by using parallelization.\n",
    "\n",
    "# Use the swifter library for parallel processing\n",
    "# data[\"text\"] = data[\"text\"].swifter.apply(spell_correction)\n",
    "\n",
    "# as it is taking more time to demonstrate so we will verify if function is correct by applying on only 1st 10 reocrds\n",
    "\n",
    "# Select the first 10 records for processing\n",
    "\n",
    "import textblob\n",
    "import swifter\n",
    "from textblob import TextBlob\n",
    "\n",
    "def spell_correction(text):\n",
    "    return str(TextBlob(text).correct())\n",
    "\n",
    "# Select the first 1000 records for processing\n",
    "subset_data = data.head(1000).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313a30766d3442a3bb6dcc242664e658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use the swifter library for parallel processing\n",
    "subset_data.loc[:, \"text\"] = subset_data[\"text\"].swifter.apply(spell_correction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180    115769 what going on with your pp its totally ...\n",
       "540    115887 him can you try starting your device by...\n",
       "961    116025 if you still can see this please do us ...\n",
       "563    britishairways i assume that being sapphire do...\n",
       "349    bought an 115821 echo show and it won’t recogn...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# removing stop words\n",
    "# 6.  stopwords\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text, language='english'):\n",
    "    # Load stopwords for the specified language\n",
    "    stop_words = set(nltk.corpus.stopwords.words(language))\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra spaces\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2615136    778161 i’d like take closer look could please ...\n",
       "1775007    frustrating local tesco wont take £20 note 540...\n",
       "1860419    500058 hey brooke sorry hear quality order les...\n",
       "225271     applesupport iphone 7 plus black screen phone ...\n",
       "2154587    671608 hi zuri kindly advise affected account ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will tokaniize the words:\n",
    "# 1.  Tokenization\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the function for tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we will doing steemming and lemmetization\n",
    "# 8.  steemming and lemmetization\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function for stemming using NLTK's PorterStemmer\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Function for lemmatization using TextBlob\n",
    "def lemmatize_text(text):\n",
    "    # Join the list of words into a string before creating a TextBlob\n",
    "    blob = TextBlob(' '.join(text))\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "    return ' '.join(lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the \"text\" column and update it\n",
    "#data[\"text\"] = data[\"text\"].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the \"text\" column and update it\n",
    "data[\"text\"] = data[\"text\"].apply(lemmatize_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
