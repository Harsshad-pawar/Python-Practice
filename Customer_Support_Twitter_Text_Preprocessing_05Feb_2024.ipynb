{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AS PART of DATA preprocessing we will be focusing on below data clearning techniques:\n",
    "\n",
    "1.  tokenization\n",
    "2.  lowercase / uppercase\n",
    "3.  emojis\n",
    "4.  pancuations\n",
    "5.  html,url\n",
    "6.  stopwords\n",
    "7.  abbravation or slang\n",
    "8.  steemming and lemmetization\n",
    "9.  spelling correction\n",
    "10. whitspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data from the Kaggle dataset\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"testdata/twcs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 22:10:47 +0000 2017</td>\n",
       "      <td>@115712 I understand. I would like to assist y...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:11:45 +0000 2017</td>\n",
       "      <td>@sprintcare and how do you propose we do that</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 22:08:27 +0000 2017</td>\n",
       "      <td>@sprintcare I have sent several private messag...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Oct 31 21:54:49 +0000 2017</td>\n",
       "      <td>@115712 Please send us a Private Message so th...</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>115712</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Oct 31 21:49:35 +0000 2017</td>\n",
       "      <td>@sprintcare I did.</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id   author_id  inbound                      created_at  \\\n",
       "0         1  sprintcare    False  Tue Oct 31 22:10:47 +0000 2017   \n",
       "1         2      115712     True  Tue Oct 31 22:11:45 +0000 2017   \n",
       "2         3      115712     True  Tue Oct 31 22:08:27 +0000 2017   \n",
       "3         4  sprintcare    False  Tue Oct 31 21:54:49 +0000 2017   \n",
       "4         5      115712     True  Tue Oct 31 21:49:35 +0000 2017   \n",
       "\n",
       "                                                text response_tweet_id  \\\n",
       "0  @115712 I understand. I would like to assist y...                 2   \n",
       "1      @sprintcare and how do you propose we do that               NaN   \n",
       "2  @sprintcare I have sent several private messag...                 1   \n",
       "3  @115712 Please send us a Private Message so th...                 3   \n",
       "4                                 @sprintcare I did.                 4   \n",
       "\n",
       "   in_response_to_tweet_id  \n",
       "0                      3.0  \n",
       "1                      1.0  \n",
       "2                      4.0  \n",
       "3                      5.0  \n",
       "4                      6.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2811769</th>\n",
       "      <td>2987947</td>\n",
       "      <td>sprintcare</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Nov 22 08:43:51 +0000 2017</td>\n",
       "      <td>@823869 Hey, we'd be happy to look into this f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2987948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811770</th>\n",
       "      <td>2987948</td>\n",
       "      <td>823869</td>\n",
       "      <td>True</td>\n",
       "      <td>Wed Nov 22 08:35:16 +0000 2017</td>\n",
       "      <td>@115714 wtf!? Iâ€™ve been having really shitty s...</td>\n",
       "      <td>2987947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811771</th>\n",
       "      <td>2812240</td>\n",
       "      <td>121673</td>\n",
       "      <td>True</td>\n",
       "      <td>Thu Nov 23 04:13:07 +0000 2017</td>\n",
       "      <td>@143549 @sprintcare You have to go to https://...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2812239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811772</th>\n",
       "      <td>2987949</td>\n",
       "      <td>AldiUK</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Nov 22 08:31:24 +0000 2017</td>\n",
       "      <td>@823870 Sounds delicious, Sarah! ðŸ˜‹ https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2987950.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811773</th>\n",
       "      <td>2987950</td>\n",
       "      <td>823870</td>\n",
       "      <td>True</td>\n",
       "      <td>Tue Nov 21 22:01:04 +0000 2017</td>\n",
       "      <td>@AldiUK  warm sloe gin mince pies with ice cre...</td>\n",
       "      <td>2987951,2987949</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   author_id  inbound                      created_at  \\\n",
       "2811769   2987947  sprintcare    False  Wed Nov 22 08:43:51 +0000 2017   \n",
       "2811770   2987948      823869     True  Wed Nov 22 08:35:16 +0000 2017   \n",
       "2811771   2812240      121673     True  Thu Nov 23 04:13:07 +0000 2017   \n",
       "2811772   2987949      AldiUK    False  Wed Nov 22 08:31:24 +0000 2017   \n",
       "2811773   2987950      823870     True  Tue Nov 21 22:01:04 +0000 2017   \n",
       "\n",
       "                                                      text response_tweet_id  \\\n",
       "2811769  @823869 Hey, we'd be happy to look into this f...               NaN   \n",
       "2811770  @115714 wtf!? Iâ€™ve been having really shitty s...           2987947   \n",
       "2811771  @143549 @sprintcare You have to go to https://...               NaN   \n",
       "2811772  @823870 Sounds delicious, Sarah! ðŸ˜‹ https://t.c...               NaN   \n",
       "2811773  @AldiUK  warm sloe gin mince pies with ice cre...   2987951,2987949   \n",
       "\n",
       "         in_response_to_tweet_id  \n",
       "2811769                2987948.0  \n",
       "2811770                      NaN  \n",
       "2811771                2812239.0  \n",
       "2811772                2987950.0  \n",
       "2811773                      NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>inbound</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>response_tweet_id</th>\n",
       "      <th>in_response_to_tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1891646</th>\n",
       "      <td>2047100</td>\n",
       "      <td>Postmates_Help</td>\n",
       "      <td>False</td>\n",
       "      <td>Wed Oct 18 23:14:58 +0000 2017</td>\n",
       "      <td>@605417 Oh no, so sorry for that mess! Please ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2047101.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997079</th>\n",
       "      <td>2154257</td>\n",
       "      <td>AmazonHelp</td>\n",
       "      <td>False</td>\n",
       "      <td>Mon Nov 20 16:06:30 +0000 2017</td>\n",
       "      <td>@632928 Thank you for your kind words. Alexa i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2154258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008735</th>\n",
       "      <td>2166122</td>\n",
       "      <td>635747</td>\n",
       "      <td>True</td>\n",
       "      <td>Sun Nov 26 14:31:27 +0000 2017</td>\n",
       "      <td>@AmazonHelp Thanks. Just sent an email</td>\n",
       "      <td>2166123</td>\n",
       "      <td>2166121.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2739712</th>\n",
       "      <td>2915056</td>\n",
       "      <td>AirAsiaSupport</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Nov 28 23:02:50 +0000 2017</td>\n",
       "      <td>@806909 we have replied you via DM.Thanks-Emir</td>\n",
       "      <td>2915057,2915058</td>\n",
       "      <td>2915059.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1875803</th>\n",
       "      <td>1742781</td>\n",
       "      <td>Tesco</td>\n",
       "      <td>False</td>\n",
       "      <td>Tue Nov 07 14:28:35 +0000 2017</td>\n",
       "      <td>@525696 Please could you return the product to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1742779.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id       author_id  inbound                      created_at  \\\n",
       "1891646   2047100  Postmates_Help    False  Wed Oct 18 23:14:58 +0000 2017   \n",
       "1997079   2154257      AmazonHelp    False  Mon Nov 20 16:06:30 +0000 2017   \n",
       "2008735   2166122          635747     True  Sun Nov 26 14:31:27 +0000 2017   \n",
       "2739712   2915056  AirAsiaSupport    False  Tue Nov 28 23:02:50 +0000 2017   \n",
       "1875803   1742781           Tesco    False  Tue Nov 07 14:28:35 +0000 2017   \n",
       "\n",
       "                                                      text response_tweet_id  \\\n",
       "1891646  @605417 Oh no, so sorry for that mess! Please ...               NaN   \n",
       "1997079  @632928 Thank you for your kind words. Alexa i...               NaN   \n",
       "2008735             @AmazonHelp Thanks. Just sent an email           2166123   \n",
       "2739712     @806909 we have replied you via DM.Thanks-Emir   2915057,2915058   \n",
       "1875803  @525696 Please could you return the product to...               NaN   \n",
       "\n",
       "         in_response_to_tweet_id  \n",
       "1891646                2047101.0  \n",
       "1997079                2154258.0  \n",
       "2008735                2166121.0  \n",
       "2739712                2915059.0  \n",
       "1875803                1742779.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@115712 I understand. I would like to assist you. We would need to get you into a private secured link to further assist.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will start with the lowercase conversion:\n",
    "# 2.  lowercase / uppercase\n",
    "\n",
    "data[\"text\"]=data[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553582     @uber_support\\nno customer support no. no emai...\n",
       "2794555    @236733 i am sorry that your package arrived i...\n",
       "2111888    @marksandspencer we did it and we loved it! no...\n",
       "2236442    first time they all flew  @virginatlantic and ...\n",
       "1361862    @467998 @115911 did you get yours early or is ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will now remove html tags and urls:\n",
    "# 5.  html,url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text1):\n",
    "    pattern=re.compile('<.*?>')\n",
    "    return pattern.sub(\"\",text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2032629    @641334 hey leland! to stop it from displaying...\n",
       "1215296    @115821 - can you tell me why a prime order fo...\n",
       "1791326    @578250 i can understand if you feel this way....\n",
       "2645242    @785459 i understand that issues like these ha...\n",
       "2666479    @790261 thanks, i'm getting that pulled up now...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1968233    @625728 a damsel in distress! ðŸ˜² share this wit...\n",
       "2167581    @165302 @coxhelp can you get century link fibe...\n",
       "854261     @sainsburys have told me \"sainsbury's do not a...\n",
       "1860518    my life is made. cheese and cold meat combos f...\n",
       "2488458    @sw_help has anyone told you there's a rugby m...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will remove the punctuation:\n",
    "#4.  punctuation\n",
    "\n",
    "import string\n",
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,\"\")\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1827441    so to southwestair for bein a homie when i dou...\n",
       "1178829                     115955 your serivces fucken suck\n",
       "2079247    652761 im sorry to hear of this parcels should...\n",
       "716777     gwrhelp hello number of carriages on the 1723 ...\n",
       "2690866    hulusupport i tried logging in to contact abou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will be removing Emojis:\n",
    "# 3.  Emojis\n",
    "\n",
    "import emoji\n",
    "def remove_emoji(text):\n",
    "    clean_text=emoji.demojize(text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301275          amazonhelp still need other orders cancelled\n",
       "517323     257430 wed be happy to answer any questions we...\n",
       "1108723    407556 please dont provide your order details ...\n",
       "1370441    470071 hey we have the rights to offer new epi...\n",
       "1668853           dropboxsupport its working fine now thanks\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AS we done data cleaning for punchuation earlier we will rerun it to remove the : and _ from the text\n",
    "data[\"text\"]=data[\"text\"].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2664817    askplaystation hi i activated a ps plus subscr...\n",
       "674488     300435 hello we have found some documentation ...\n",
       "650752     294498 were sorry to hear about the trouble ch...\n",
       "1096528             delta i asked air china to add my number\n",
       "1004990    382821 also donâ€™t provide your order details a...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1763804    you guys may think its stupid but im happy and...\n",
       "1316515    amazonhelp hello can i have the uk fax number ...\n",
       "1905759    605529 gotcha try unplugging the box and unscr...\n",
       "286625     194109 hi nicole glad to see youre back with u...\n",
       "1616327    532259 well be happy to take a closer look sen...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will update abbravation or slang words:\n",
    "# 7.  abbravation\n",
    "\n",
    "chat_words={\n",
    "   \" AFAIK\":\"As Far As I Know\",\n",
    "\"AFK\": \"Away From Keyboard\",\n",
    "\"ASAP\":\"As Soon As Possible\",\n",
    "\"BTW\":\"By The Way\",\n",
    "\"B4\":\"Before\",\n",
    "\"LAMO\":\"Laugh My A.. Off\",\n",
    "\"FYI\":\"For your information\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    \n",
    "    new_text=[]\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[\"text\"] = data[\"text\"].apply(lambda x: ' '.join(x))\n",
    "data[\"text\"] = data[\"text\"].apply(chat_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191982     169917 426 427 81 169918 169919 160959 428 429...\n",
       "503840     159776 hi just stepping in to see if we could ...\n",
       "713580     atviassist thank you for responding but i stil...\n",
       "2594440                     sprintcare data fucking up again\n",
       "712264     115911 117159 cont i have the jump off plan to...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will rectify spelling mistakes:\n",
    "# 6.  spelling mistakes\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "def spell_correction(text):\n",
    "    return str(TextBlob(text).correct())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: spell_correction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text, language='english'):\n",
    "    # Load stopwords for the specified language\n",
    "    stop_words = set(nltk.corpus.stopwords.words(language))\n",
    "\n",
    "    words = text.split()\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra spaces\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2811774"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834547     340516 provide info conditions carriage bookin...\n",
       "242121     amazonhelp 1840 115851 please tell cheated ama...\n",
       "1180212    424602 apologies expected wither please dm us ...\n",
       "745194     318340 hello sorry hear dm us phone number att...\n",
       "1325271    458893 sorry hear ashley anything assist pleas...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the updated 'data' DataFrame\n",
    "data.sample(5)[\"text\"]\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# now we will tokaniize the words:\n",
    "# 1.  Tokenization\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define the function for tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"]=data[\"text\"].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1889819    [604900, i, â€™, d, like, share, thoughts, pass,...\n",
       "2218858                          [southwestair, thanks, lot]\n",
       "743255     [115851, sir, support, 115850, tracing, item, ...\n",
       "1538341    [512629, boy, ever, check, find, perfect, one,...\n",
       "1627494    [535326, shanna, let, look, fees, please, dm, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At last we will doing steemming and lemmetization\n",
    "# 8.  steemming and lemmetization\n",
    "\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Function for stemming using NLTK's PorterStemmer\n",
    "def stem_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in text.split()]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Function for lemmatization using TextBlob\n",
    "def lemmatize_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    lemmatized_words = [word.lemmatize() for word in blob.words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply stemming to the \"text\" column and update it\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# as we have less time we will go with steming only\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\pandas\\core\\series.py:4904\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4771\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4777\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4778\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4779\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4780\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4895\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4896\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4898\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4902\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[66], line 12\u001b[0m, in \u001b[0;36mstem_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem_text\u001b[39m(text):\n\u001b[0;32m     11\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 12\u001b[0m     stemmed_words \u001b[38;5;241m=\u001b[39m [stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_words)\n",
      "Cell \u001b[1;32mIn[66], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstem_text\u001b[39m(text):\n\u001b[0;32m     11\u001b[0m     stemmer \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m---> 12\u001b[0m     stemmed_words \u001b[38;5;241m=\u001b[39m [\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(stemmed_words)\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\nltk\\stem\\porter.py:671\u001b[0m, in \u001b[0;36mPorterStemmer.stem\u001b[1;34m(self, word, to_lowercase)\u001b[0m\n\u001b[0;32m    669\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step1a(stem)\n\u001b[0;32m    670\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step1b(stem)\n\u001b[1;32m--> 671\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step1c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    672\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step2(stem)\n\u001b[0;32m    673\u001b[0m stem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step3(stem)\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\nltk\\stem\\porter.py:422\u001b[0m, in \u001b[0;36mPorterStemmer._step1c\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moriginal_condition\u001b[39m(stem):\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contains_vowel(stem)\n\u001b[1;32m--> 422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_rule_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnltk_condition\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNLTK_EXTENSIONS\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moriginal_condition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mh:\\Github Repos\\GenAI-Course\\Python Practice\\pyenv\\lib\\site-packages\\nltk\\stem\\porter.py:259\u001b[0m, in \u001b[0;36mPorterStemmer._apply_rule_list\u001b[1;34m(self, word, rules)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m rules:\n\u001b[0;32m    258\u001b[0m     suffix, replacement, condition \u001b[38;5;241m=\u001b[39m rule\n\u001b[1;32m--> 259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msuffix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ends_double_consonant(word):\n\u001b[0;32m    260\u001b[0m         stem \u001b[38;5;241m=\u001b[39m word[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m condition(stem):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply stemming to the \"text\" column and update it\n",
    "data[\"text\"] = data[\"text\"].apply(stem_text)\n",
    "# as we have less time we will go with steming only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208780    argoshelpers thanks chance detailed informatio...\n",
       "731214     314680 thanks reaching unfortunately can â€™ t r...\n",
       "2041280    643414 ohh looks delish enjoy flight thank cho...\n",
       "2636800            comcastoutage comcastcares explain outage\n",
       "927359     363538 wed love help issue coming across devic...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the \"text\" column and update it\n",
    "data[\"text\"] = data[\"text\"].apply(lemmatize_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
