{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, AutoTokenizer\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from whisper_mic import WhisperMic\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device and torch dtype for the model on the GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b3fd0d8eb64345aa2301d11020a9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/20/24 21:17:38] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> No mic index provided, using default                                 <a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">whisper_mic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#84\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">84</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/20/24 21:17:38]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m No mic index provided, using default                                 \u001b]8;id=285394;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\u001b\\\u001b[2mwhisper_mic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=614763;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#84\u001b\\\u001b[2m84\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[02/20/24 21:17:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Mic setup complete                                                   <a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">whisper_mic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[02/20/24 21:17:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Mic setup complete                                                   \u001b]8;id=533329;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\u001b\\\u001b[2mwhisper_mic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=742851;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Listening<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                                        <a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">whisper_mic.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#214\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">214</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Listening\u001b[33m...\u001b[0m                                                        \u001b]8;id=343058;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py\u001b\\\u001b[2mwhisper_mic.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=114617;file://c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\whisper_mic\\whisper_mic.py#214\u001b\\\u001b[2m214\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What is the capital of India?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    mic = WhisperMic()\n",
    "    result = mic.listen()\n",
    "    print(result)\n",
    "    promptinput = result\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY2\"))\n",
    "\n",
    "\n",
    "question = promptinput\n",
    "\n",
    "# Call the OpenAI API to get a response to the user's question\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"assistant\", \"content\": \"You are a helpful assistant designed to answer user queries.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the generated text from the response\n",
    "print(completion.choices[0].message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0253d77b17224b5fb7588eacb762b1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "# as we are able to convert the speech to text, we can now use the text to text model\n",
    "\n",
    "\n",
    "t2tmodel=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer=AutoTokenizer.from_pretrained(t2tmodel)\n",
    " # create Huggingface pipeline\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\",\n",
    "     model=t2tmodel,\n",
    "     tokenizer=tokenizer,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     trust_remote_code=True,\n",
    "     device_map=\"auto\",\n",
    "     max_length=1000,\n",
    "     do_sample=True,\n",
    "     top_k=10,\n",
    "     num_return_sequences=1,\n",
    "     eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "print(t2tmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x000002C77CAACD90>\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2tresult=llm.invoke(promptinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_prompt = \"Write a creative story about...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2tresult = llm.invoke(prompt=zero_shot_prompt,input=promptinput, model_kwargs={'temperature':0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A. Mumbai\n",
      "B. Delhi\n",
      "C. Kolkata\n",
      "D. Chennai\n",
      "\n",
      "Answer: B. Delhi\n",
      "\n",
      "Explanation: Delhi is the capital city of India.\n"
     ]
    }
   ],
   "source": [
    "print(t2tresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to audio\n",
    "import playsound\n",
    "from gtts import gTTS\n",
    "tts = gTTS(text=t2tresult, lang='en')  # Replace 'en' with your desired language\n",
    "tts.save('output.mp3')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the audio file\n",
    "playsound(\"generated_audio.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
