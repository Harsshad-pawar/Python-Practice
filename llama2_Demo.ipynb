{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e3c411a5e64701b00b15e15245f871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the required Libraries\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import huggingface_pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Llama 2 Model\n",
    "\n",
    "model=\"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1dd93831364ff1ba25bd15d58d2e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    " # create Huggingface pipeline\n",
    "\n",
    " pipeline=transformers.pipeline(\n",
    "     \"text-generation\",\n",
    "     model=model,\n",
    "     tokenizer=tokenizer,\n",
    "     torch_dtype=torch.bfloat16,\n",
    "     trust_remote_code=True,\n",
    "     device_map=\"auto\",\n",
    "     max_length=1000,\n",
    "     do_sample=True,\n",
    "     top_k=10,\n",
    "     num_return_sequences=1,\n",
    "     eos_token_id=tokenizer.eos_token_id\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x0000023A9F1ECD30>\n"
     ]
    }
   ],
   "source": [
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipeline, model_kwargs={'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"What would be a good name for a company that makes colorful socks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using CPU.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "?\n",
      "\n",
      "Answer: A good name for a company that makes colorful socks could be something playful and catchy, like:\n",
      "\n",
      "1. SoleMates: This name plays on the idea of socks being your \"sole\" mates, or your best friends.\n",
      "2. HueHues: This name is a play on the word \"hue,\" which means a color, and could be a fun and quirky name for a sock company.\n",
      "3. Footloose & Fancy-Free: This name is a play on the phrase \"footloose and fancy-free,\" which means feeling carefree and happy. It could be a fun name for a company that wants to convey a sense of fun and freedom.\n",
      "4. The Sock Syndicate: This name has a fun, playful sound to it, and could work well for a company that wants to convey a sense of fun and creativity.\n",
      "5. The Sock Society: This name has a similar feel to \"The Sock Syndicate,\" but with a slightly more serious tone. It could work well for a company that wants to convey a sense of sophistication and style.\n",
      "6. Socktastic: This name is a play on the word \"fantastic,\" and could be a fun and catchy name for a sock company.\n",
      "7. The Sock Studio: This name has a creative and artistic feel to it, and could work well for a company that wants to convey a sense of innovation and design.\n",
      "8. The Sock Shop: This name is simple and straightforward, and could work well for a company that wants to convey a sense of reliability and consistency.\n",
      "\n",
      "I hope these suggestions help inspire you as you come up with a name for your sock company!\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "librosa version: 0.10.1\n",
      "soundfile version: 0.12.1\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "\n",
    "print(f\"librosa version: {librosa.__version__}\")\n",
    "print(f\"soundfile version: {soundfile.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testaudio1.m4a\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory containing your files\n",
    "directory = \"H:/Github Repos/GenAI-Course/Python Practice/testdata\"\n",
    "\n",
    "# List files in the directory\n",
    "files = os.listdir(directory)\n",
    "print(files[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\harsh\\anaconda3\\envs\\development\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:697: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating opportunities for our team to innovate and to drive the business forward. CBI Leadership Behaviors Resolving disagreements directly with constructive dialogue, curiosity and compassion. CBI Leadership Behaviors 5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set device and torch dtype\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# Specify file path and model ID\n",
    "audio_path = \"H:/audio1.mp3\"\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# Load the model and processor\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# Create the pipeline\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(audio_path):\n",
    "    print(f\"Error: Audio file not found: {audio_path}\")\n",
    "    exit()\n",
    "\n",
    "# Process the audio and get the transcribed text\n",
    "try:\n",
    "    result = pipe(audio_path)\n",
    "    print(result[\"text\"])\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    print(\"Please refer to the documentation or seek further assistance.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "development",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
